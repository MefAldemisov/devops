# Lab 13

1. Output of the `kubectl get po,sts,svc,pvc`:
```                                                               Mon Oct  4 22:26:06 2021
NAME               READY   STATUS    RESTARTS   AGE
pod/app-python-0   1/1     Running   0          9h
pod/app-python-1   1/1     Running   0          9h
pod/app-python-2   1/1     Running   0          9h

NAME                          READY   AGE
statefulset.apps/app-python   3/3     9h

NAME                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/app-python   LoadBalancer   10.103.35.53   <pending>     5000:30911/TCP   9h
service/kubernetes   ClusterIP      10.96.0.1      <none>        443/TCP          12h

NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/counter-app-python-0   Bound    pvc-91bfcb7f-59c3-4b10-a598-5f8f4e871841   128Mi      RWO            standard       10h
persistentvolumeclaim/counter-app-python-1   Bound    pvc-17f1896b-ae11-4447-aae8-48a017e2740f   128Mi      RWO            standard       10h
persistentvolumeclaim/counter-app-python-2   Bound    pvc-9a3a7950-db8e-4ff6-b83f-d21452d20ea3   128Mi      RWO            standard       10h
```
2. The load was distributed among pods randomly (tab1 accessed 7 times, tab 2 - 10 and the last - 15)
```
> kubectl exec pod/app-python-0 -- cat /app/data/visits.txt
13⏎                                                                                                                                                                         
> kubectl exec pod/app-python-1 -- cat /app/data/visits.txt 
10⏎                                                                                                                                                                         
>kubectl exec pod/app-python-2 -- cat /app/data/visits.txt
9⏎                                                                                                     
```
3. Why is it so?
- StatefulSet => state, which is unique for each pod
- `Values.servic.LoadBalancer` => balancing between nodes

4. For our app ordering guarantee are unnecessary. Describe in the report why. Then find a way to tell to
the StatefulSet controller to launch or terminate all Pods in parallel. Implement it.
- **Ordering guarantee**: the ordering is important, when pods are not independent. For instance, when the database, 
backend and frontend will be executed on different pods. However, in our app pods are equal and independent, thus, the ordering doesn't matter.
- **To launch and terminate in parallel**: specify `podManagementPolicy: "Parallel"` in `values.yaml` (actually used in `statefulset.yaml`)
